{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "literary-sunglasses",
   "metadata": {},
   "source": [
    "# New test set \n",
    "The aim of this notebook is to create a new test method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-anime",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hydraulic-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utils librairies ---\n",
    "#Generics librairies\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Measure librairies\n",
    "import time\n",
    "\n",
    "#Dataset librairies\n",
    "import pandas as pd\n",
    "\n",
    "# --- DL librairies ---\n",
    "#Pytorch librairies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Sklearn librairies\n",
    "from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-segment",
   "metadata": {},
   "source": [
    "## GPU environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bizarre-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4\";\n",
    "gpus_list = [0, 1, 2, 3]\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-houston",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-queen",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, labels, names = pd.read_pickle(\"../data/dataset_COVID_RAW.pkl\")\n",
    "df = pd.DataFrame(df)\n",
    "labels = pd.Series(labels, name = 'label')\n",
    "names = pd.Series(names, name = 'names')\n",
    "df = pd.concat((df, labels, names), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CovNeg_26\n"
     ]
    }
   ],
   "source": [
    "outside_group = np.random.choice(df.names.values)\n",
    "print(outside_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_set = df[df.names.values == outside_group]\n",
    "df = df[df.names.values != outside_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handled-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set = df.drop(columns = ['label', \"names\"]).values\n",
    "Y_set = df.label.values\n",
    "groups = df.names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(LeaveOneGroupOut().split(X_set, Y_set, groups = groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-alert",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aerial-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAugment(signal, betashift = 0.24039033704204857, slopeshift = 0.5640435054299953, multishift = 0.0013960388613510225):\n",
    "    #baseline shift\n",
    "    beta = np.random.random(size=(signal.shape[0],1))*2*betashift-betashift\n",
    "    slope = np.random.random(size=(signal.shape[0],1))*2*slopeshift-slopeshift + 1\n",
    "    #relative positions\n",
    "    axis = np.array(range(signal.shape[1]))/float(signal.shape[1])\n",
    "    #offset\n",
    "    offset = slope*(axis) + beta - axis - slope/2. + 0.5\n",
    "\n",
    "    #multiplicative coefficient\n",
    "    multi = np.random.random(size=(signal.shape[0],1))*2*multishift-multishift + 1\n",
    "    augmented_signal = multi*signal + offset\n",
    "\n",
    "    return augmented_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-lincoln",
   "metadata": {},
   "source": [
    "### Creation of sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "israeli-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RamanDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super(RamanDataset).__init__()\n",
    "        x = torch.from_numpy(X)\n",
    "        self.raman_spectra = x\n",
    "        y = torch.from_numpy(Y)\n",
    "        self.labels = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raman_spectra)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        spectrum = self.raman_spectra[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return spectrum, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "silent-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, ramanDatasets):\n",
    "        super(Datasets).__init__()\n",
    "        self.datasets = ramanDatasets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datasets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.datasets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "historic-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setsCreation(pathToFile, X_set, Y_set, folds):\n",
    "    if not path.exists(pathToFile):\n",
    "        train_set = []\n",
    "        validation_set = []\n",
    "        test_set = []\n",
    "        X_train = []\n",
    "        X_val = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_val = []\n",
    "        Y_test = []\n",
    "        \n",
    "        for i, (train_idx, test_idx) in enumerate(folds):\n",
    "            X_train_tmp = X_set[train_idx]\n",
    "            Y_train_tmp = Y_set[train_idx]\n",
    "            \n",
    "            X_test_tmp = X_set[test_idx]\n",
    "            Y_test_tmp = Y_set[test_idx]\n",
    "            \n",
    "            X_train_tmp, X_val_tmp, Y_train_tmp, Y_val_tmp = train_test_split(X_train_tmp, Y_train_tmp, test_size = .1,\n",
    "                                                                              stratify = Y_train_tmp)\n",
    "            augment = 30\n",
    "            augmented_data = []\n",
    "            Y_list = copy.copy(Y_train_tmp)\n",
    "            for i in range(augment):\n",
    "                augmented_data.append(dataAugment(X_train_tmp))\n",
    "            for i in range(augment-1):\n",
    "                Y_list = np.concatenate((Y_list, Y_train_tmp), axis=0)\n",
    "                \n",
    "            X_train_tmp = np.vstack(augmented_data)\n",
    "            Y_train_tmp = copy.copy(Y_list)\n",
    "            train_set_tmp = RamanDataset(X_train_tmp, Y_train_tmp)\n",
    "            train_set.append(train_set_tmp)\n",
    "            val_set_tmp = RamanDataset(X_val_tmp, Y_val_tmp)\n",
    "            validation_set.append(val_set_tmp)\n",
    "            test_set_tmp = RamanDataset(X_test_tmp, Y_test_tmp)\n",
    "            test_set.append(test_set_tmp)\n",
    "        \n",
    "        train_dataset = Datasets(train_set)\n",
    "        validation_dataset = Datasets(validation_set)\n",
    "        test_dataset = Datasets(test_set)\n",
    "        training_settings = (train_dataset, validation_dataset, test_dataset)\n",
    "        \n",
    "        with open(pathToFile, \"wb\") as outf:\n",
    "            pickle.dump(training_settings, outf)\n",
    "        \n",
    "    else:\n",
    "        with open(pathToFile, \"rb\") as inf:\n",
    "            train_dataset, validation_dataset, test_dataset = pickle.load(inf)\n",
    "            \n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sensitive-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToFile = \"../train_settings/training_settings_cov_raw_without_\" + str(outside_group) + \".pckl\"\n",
    "train_dataset, validation_dataset, test_dataset = setsCreation(pathToFile, X_set, Y_set, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attempted-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_set = final_test_set.drop(columns = ['label', \"names\"]).values\n",
    "final_Y_set = final_test_set.label.values\n",
    "test_set_final = RamanDataset(final_X_set, final_Y_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-response",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-throw",
   "metadata": {},
   "source": [
    "### Utility classes and functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assumed-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 100, kernel_size=100,\n",
    "                     stride=1, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100, eps=0.001, momentum=0.99),\n",
    "            nn.Conv1d(100, 102, kernel_size=5,\n",
    "                     stride=2, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(6, stride=3),\n",
    "            nn.BatchNorm1d(102, eps=0.001, momentum=0.99),\n",
    "            nn.Conv1d(102, 25, kernel_size=9,\n",
    "                     stride=5, padding_mode='replicate'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(325, 732),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.7000000000000001),\n",
    "            nn.Linear(732, 152),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(152,189),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(189, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.resize_(x.shape[0],  1, x.shape[1])\n",
    "        x = self.cnn_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dense_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rapid-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNN(gpu_ids):\n",
    "    model = ConvNet()\n",
    "    model = model.double()\n",
    "    optimizer = Adam(model.parameters(), lr=0.00020441990333108206)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        cuda='cuda:'+str(gpu_ids[0])\n",
    "        model = nn.DataParallel(model, device_ids=gpu_ids)\n",
    "        loss.cuda()\n",
    "    device = torch.device(cuda if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    return model, loss, optimizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sealed-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, loss, optimizer, train_dataset, validation_dataset, epochs, patience, path, verbose=0, batch_size=338):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    min_val_loss = np.Inf\n",
    "    max_val_acc = np.NINF\n",
    "    epochs_no_improve_loss = 0\n",
    "    epochs_no_improve_acc = 0\n",
    "    if verbose == 1:\n",
    "        verbScheduler = True\n",
    "    else:\n",
    "        verbScheduler = False\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=80, cooldown=10, verbose=verbScheduler)\n",
    "    training_generator = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    validation_generator = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss_train_epoch = []\n",
    "        acc_train_epoch = []\n",
    "        for i, (ramanSpectraTrain, labelTrain) in enumerate(training_generator):\n",
    "            ramanSpectraTrain = ramanSpectraTrain.to(device)\n",
    "            labelTrain = labelTrain.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            output_train = model(ramanSpectraTrain)\n",
    "            \n",
    "            loss_train = loss(output_train, labelTrain)\n",
    "            loss_train_epoch.append(loss_train.cpu().item())\n",
    "            \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            output_label = torch.argmax(output_train, dim=1)\n",
    "            acc_train = accuracy_score(labelTrain.cpu().detach().numpy(), output_label.cpu().detach().numpy())\n",
    "            acc_train_epoch.append(acc_train)\n",
    "        \n",
    "        loss_train = mean(loss_train_epoch)\n",
    "        acc_train = mean(acc_train_epoch)\n",
    "        train_losses.append(loss_train)\n",
    "        train_acc.append(acc_train)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_val_epoch = []\n",
    "            acc_val_epoch = []\n",
    "            for j, (ramanSpectraVal, labelVal) in enumerate(validation_generator):\n",
    "                ramanSpectraVal = ramanSpectraVal.to(device)\n",
    "                labelVal = labelVal.to(device)\n",
    "                    \n",
    "                output_val = model(ramanSpectraVal)\n",
    "                    \n",
    "                loss_val = loss(output_val, labelVal)\n",
    "                loss_val_epoch.append(loss_val.cpu().item())\n",
    "                \n",
    "                val_label = torch.argmax(output_val, dim=1)\n",
    "                acc_val = accuracy_score(labelVal.cpu().detach().numpy(), val_label.cpu().detach().numpy())\n",
    "                acc_val_epoch.append(acc_val)\n",
    "            \n",
    "            loss_val = mean(loss_val_epoch)\n",
    "            acc_val = mean(acc_val_epoch)\n",
    "        val_losses.append(loss_val)\n",
    "        val_acc.append(acc_val)\n",
    "        scheduler.step(loss_val)\n",
    "        if acc_val > max_val_acc:\n",
    "            epochs_no_improve_acc = 0\n",
    "            max_val_acc = acc_val\n",
    "            torch.save({'model_state_dict' : model.state_dict(),\n",
    "                       'optimizer_state_dict' : optimizer.state_dict(),\n",
    "                       'train_loss' : train_losses,\n",
    "                       'train_acc' : train_acc,\n",
    "                       'val_loss' : val_losses,\n",
    "                       'val_acc' : val_acc}, path)\n",
    "        else:\n",
    "            epochs_no_improve_acc += 1\n",
    "        \n",
    "        if loss_val < min_val_loss:\n",
    "            epochs_no_improve_loss = 0\n",
    "            min_val_loss = loss_val\n",
    "            torch.save({'model_state_dict' : model.state_dict(),\n",
    "                       'optimizer_state_dict' : optimizer.state_dict(),\n",
    "                       'train_loss' : train_losses,\n",
    "                       'train_acc' : train_acc,\n",
    "                       'val_loss' : val_losses,\n",
    "                       'val_acc' : val_acc}, path)\n",
    "        else:\n",
    "            epochs_no_improve_loss += 1\n",
    "            \n",
    "        if verbose == 1:\n",
    "            print(\"Epoch {}:\\t train loss : {}; train accuracy : {}; \\n validation loss : {}; validation accuracy : {}\".format(epoch+1, loss_train, acc_train, loss_val, acc_val))\n",
    "            \n",
    "        if epochs_no_improve_loss >= patience and epochs_no_improve_acc >= patience:\n",
    "            print(\"Early stopping at epoch {}\".format(epoch+1))\n",
    "            break\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_loss']\n",
    "    train_acc = checkpoint['train_acc']\n",
    "    val_losses = checkpoint['val_loss']\n",
    "    val_acc = checkpoint['val_acc']\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(\"------------------------------ Final result of the model ! ------------------------------\")\n",
    "        print(\"Train loss : {}; Train accuracy : {}; \\n Validation loss : {}; Validation accuracy : {}\".format(train_losses[-1], train_acc[-1], val_losses[-1], val_acc[-1]))\n",
    "        \n",
    "    return train_losses, val_losses, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "material-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, test_set, device, batch_size=1):\n",
    "    test_acc = []\n",
    "    test_generator = DataLoader(test_set, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    for i, (ramanSpectra, label) in enumerate(test_generator):\n",
    "        ramanSpectra = ramanSpectra.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        labelPredict = model(ramanSpectra)\n",
    "        labelPredict = torch.argmax(labelPredict, dim=1)\n",
    "        \n",
    "        acc = accuracy_score(label.cpu().detach().numpy(), labelPredict.cpu().detach().numpy())\n",
    "        test_acc.append(acc)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "north-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalTest(models, test_set, devices, batch_size=1):\n",
    "    test_generator = DataLoader(test_set, batch_size=batch_size)\n",
    "    predictions = [[] for _ in models]\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        model.eval()\n",
    "        for j, (ramanSpectra, label) in enumerate(test_generator):\n",
    "            ramanSpectra = ramanSpectra.to(devices[i])\n",
    "            label = label.to(devices[i])\n",
    "        \n",
    "            labelPredict = model(ramanSpectra)\n",
    "            labelPredict = torch.argmax(labelPredict, dim=1)\n",
    "            predictions[i].append(labelPredict)\n",
    "    \n",
    "    finalPrediction = []\n",
    "    for i in range(len(predictions[0])):\n",
    "        count0 = 0\n",
    "        count1 = 0\n",
    "        count2 = 0\n",
    "        for j in range(len(predictions)):\n",
    "            if predictions[j][i] == 0:\n",
    "                count0 += 1\n",
    "            elif predictions[j][i] == 1:\n",
    "                count1 += 1\n",
    "            else:\n",
    "                count2 += 1\n",
    "        m = max(count0, count1, count2)\n",
    "        if m == count0:\n",
    "            finalPrediction.append(0)\n",
    "        elif m == count1:\n",
    "            finalPrediction.append(1)\n",
    "        else:\n",
    "            finalPrediction.append(2)\n",
    "    \n",
    "    realLabels = []\n",
    "    for j, (ramanSpectra, label) in enumerate(test_generator):\n",
    "        realLabels.append(label)\n",
    "        \n",
    "    acc = accuracy_score(realLabels, finalPrediction)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-tumor",
   "metadata": {},
   "source": [
    "### Train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "automotive-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Let's train model 1 ! ------------------------------\n",
      "Epoch 1:\t train loss : 0.9558276391390352; train accuracy : 0.5695577701650576; \n",
      " validation loss : 0.8204464463912797; validation accuracy : 0.7383966244725738\n",
      "Epoch 2:\t train loss : 0.7624936468577672; train accuracy : 0.7866292951313194; \n",
      " validation loss : 0.7287124671127737; validation accuracy : 0.8143459915611815\n",
      "Epoch 3:\t train loss : 0.7087735630258768; train accuracy : 0.8411605937921728; \n",
      " validation loss : 0.7299723241713809; validation accuracy : 0.810126582278481\n",
      "Epoch 4:\t train loss : 0.6823444177060696; train accuracy : 0.8685404339250493; \n",
      " validation loss : 0.7087638442504923; validation accuracy : 0.8481012658227848\n",
      "Epoch 5:\t train loss : 0.6749435416795745; train accuracy : 0.8751479289940829; \n",
      " validation loss : 0.666149380895674; validation accuracy : 0.8776371308016878\n",
      "Epoch 6:\t train loss : 0.6582079929666865; train accuracy : 0.8925672168587149; \n",
      " validation loss : 0.7103150164049931; validation accuracy : 0.8396624472573839\n",
      "Epoch 7:\t train loss : 0.6504630534411014; train accuracy : 0.8999792380359182; \n",
      " validation loss : 0.645818287655926; validation accuracy : 0.9029535864978903\n",
      "Epoch 8:\t train loss : 0.6389838815027652; train accuracy : 0.9115384615384615; \n",
      " validation loss : 0.7248798726366601; validation accuracy : 0.8227848101265823\n",
      "Epoch 9:\t train loss : 0.6299867339937711; train accuracy : 0.9211460604173155; \n",
      " validation loss : 0.6592863878965282; validation accuracy : 0.890295358649789\n",
      "Epoch 10:\t train loss : 0.6222464455188541; train accuracy : 0.9288124156545209; \n",
      " validation loss : 0.6466075546404865; validation accuracy : 0.9029535864978903\n",
      "Epoch 11:\t train loss : 0.6116871786308282; train accuracy : 0.9394425412644036; \n",
      " validation loss : 0.637356187926789; validation accuracy : 0.919831223628692\n",
      "Epoch 12:\t train loss : 0.6058386969642972; train accuracy : 0.9451001764766946; \n",
      " validation loss : 0.6440798521510749; validation accuracy : 0.8945147679324894\n",
      "Epoch 13:\t train loss : 0.5997483990496933; train accuracy : 0.9518945292224644; \n",
      " validation loss : 0.6383565275321386; validation accuracy : 0.9071729957805907\n",
      "Epoch 14:\t train loss : 0.5972044260928776; train accuracy : 0.9544378698224852; \n",
      " validation loss : 0.6378832200084128; validation accuracy : 0.9156118143459916\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-321f25896b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_directory\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pckl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart_bis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mend_bis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed time for 1 model : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_bis\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_bis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e6c1c53b2b9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, model, loss, optimizer, train_dataset, validation_dataset, epochs, patience, path, verbose, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0macc_train_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 273\n",
    "patience = 50\n",
    "models_list = []\n",
    "optimizers = []\n",
    "loss_train_list = []\n",
    "loss_val_list = []\n",
    "acc_train_list = []\n",
    "acc_val_list = []\n",
    "devices = []\n",
    "start = time.time()\n",
    "path_to_directory = '../saved_models/covid_RAW_without_'+str(outside_group)\n",
    "if not(os.path.exists(path_to_directory)):\n",
    "        os.mkdir(path_to_directory)\n",
    "for i in range(len(train_dataset)):\n",
    "    print(\"------------------------------ Let's train model {} ! ------------------------------\".format(i+1))\n",
    "    model, loss, optimizer, device = createCNN(gpus_list)\n",
    "    path = path_to_directory +\"/\"+str(i+1)+\".pckl\"\n",
    "    start_bis = time.time()\n",
    "    train_loss, val_loss, train_acc, val_acc = train(device, model, loss, optimizer, train_dataset[i], validation_dataset[i], num_epochs, patience, path, verbose=1)\n",
    "    end_bis = time.time()\n",
    "    print(\"Elapsed time for 1 model : \", end_bis - start_bis)\n",
    "    devices.append(device)\n",
    "    loss_train_list.append(train_loss)\n",
    "    loss_val_list.append(val_loss)\n",
    "    acc_train_list.append(train_acc)\n",
    "    acc_val_list.append(val_acc)\n",
    "    models_list.append(model)\n",
    "    optimizers.append(optimizer)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training time :\", (end-start)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "hT = (end-start)//3600\n",
    "mT = ((end-start)%3600)//60\n",
    "sT = (((end-start)%3600)%60)\n",
    "print(\"------------------------------ Total time of training {} h {} m and {} s ------------------------------\".format(hT, mT, sT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-infrastructure",
   "metadata": {},
   "source": [
    "### Test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accs = []\n",
    "for i in range(len(test_dataset)):\n",
    "    print(\"------------------------------ Let's predict with model {} ! ------------------------------\".format(i+1))\n",
    "    acc = testModel(models_list[i], test_dataset[i], devices[i])\n",
    "    test_accs.append(mean(acc))\n",
    "    print(\"------------------------------ Model {} predict with {} of accuracy ------------------------------\".format(i+1, mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "for i in range(len(test_accs)):\n",
    "    total_acc += test_accs[i]\n",
    "print(\"The mean accuracy is {}\".format(total_acc/len(test_accs))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-auditor",
   "metadata": {},
   "source": [
    "### Global test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = globalTest(models_list, test_set_final, devices)\n",
    "print(\"The accuracy obtain on the final test set is {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
